
@inproceedings{Sukhbaatar2014TrainingCN,
  title={Training Convolutional Networks with Noisy Labels},
  author={Sainbayar Sukhbaatar and Joan Bruna and Manohar Paluri and Lubomir D. Bourdev and Rob Fergus},
  year={2014},
  booktitle = {ICLR Workshop track},
}

@inproceedings{1406.2661,
 author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 title = {Generative Adversarial Nets},
 booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'14},
 year = {2014},
 location = {Montreal, Canada},
 pages = {2672--2680},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2969033.2969125},
 acmid = {2969125},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA}
} 

@inproceedings{10.1.1.94.777,
author = {Cano-Perez, Javier and Pérez-Cortes, Juan-Carlos and Arl, Joaquim and Llobet, Rafael},
year = {2003},
month = {01},
pages = {},
booktitle = {Structural, Syntactic, and Statistical Pattern Recognition: Joint IAPR International Workshops SSPR 2002 and SPR 2002 Windsor, Ontario, Canada, August 6–9, 2002 Proceedings},
title = {Training Set Expansion in Handwritten Character Recognition}
}

@inproceedings{10.1.1.151.7688,
author = { Jean  Nonnemaker,Henry S. Baird},
title = {Using synthetic data safely in classification},
journal = {Proc.SPIE},
volume = {7247},
number = {},
pages = {7247 - 7247 - 11},
year = {2009},
doi = {10.1117/12.805619},
URL = {https://doi.org/10.1117/12.805619},
eprint = {},
month = {01},
booktitle = { PROCEEDINGS VOLUME 7247 IS\&T/SPIE ELECTRONIC IMAGING | 18-22 JANUARY 2009 }
}

@inproceedings{10.1109/CVPR.2016.442,
author={A. Handa and V. Patraucean and V. Badrinarayanan and S. Stent and R. Cipolla},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Understanding RealWorld Indoor Scenes with Synthetic Data},
year={2016},
volume={},
number={},
pages={4077-4085},
keywords={computer graphics;image colour analysis;learning (artificial intelligence);SUN RGB-D dataset;automated intelligent machine;computer graphics;depth-based segmentation;indoor scenes;scene understanding problem;semantic per-pixel labelling;supervised data;supervised learning;synthetic 3D scenes;synthetic data;Data models;Image segmentation;Labeling;Solid modeling;Sun;Three-dimensional displays;Training data},
doi={10.1109/CVPR.2016.442},
ISSN={},
month={June}
}




@InProceedings{1703.05192.pdf,
  title =  {Learning to Discover Cross-Domain Relations with Generative Adversarial Networks},
  author =  {Taeksoo Kim and Moonsu Cha and Hyunsoo Kim and Jung Kwon Lee and Jiwon Kim},
  booktitle =  {Proceedings of the 34th International Conference on Machine Learning},
  pages =  {1857--1865},
  year =  {2017},
  editor =  {Doina Precup and Yee Whye Teh},
  volume =  {70},
  series =  {Proceedings of Machine Learning Research},
  address =  {International Convention Centre, Sydney, Australia},
  month =  {06--11 Aug},
  publisher =  {PMLR},
  pdf =  {http://proceedings.mlr.press/v70/kim17a/kim17a.pdf},
  url =  {http://proceedings.mlr.press/v70/kim17a.html},
  abstract =  {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on a generative adversarial network that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.}
}

@INPROCEEDINGS{10.1109/CVPR.2017.18,
author={K. Bousmalis and N. Silberman and D. Dohan and D. Erhan and D. Krishnan},
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title={Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks},
year={2017},
volume={},
number={},
pages={95-104},
keywords={feature extraction;image annotation;image representation;learning (artificial intelligence);rendering (computer graphics);adaptation process;generative adversarial network;ground-truth annotations;map representations;modern machine learning algorithms;pixel space;rendered images;source-domain images;unsupervised domain adaptation algorithms;unsupervised domain adaptation scenarios;unsupervised pixel-level domain adaptation;well-annotated image datasets;Adaptation models;Feature extraction;Gallium nitride;Generators;Google;Training},
doi={10.1109/CVPR.2017.18},
ISSN={1063-6919},
month={July},}

@Inbook{10.1007/978-3-319-58347-1,
author="Ganin, Yaroslav
and Ustinova, Evgeniya
and Ajakan, Hana
and Germain, Pascal
and Larochelle, Hugo
and Laviolette, Fran{\c{c}}ois
and Marchand, Mario
and Lempitsky, Victor",
title="Domain-Adversarial Training of Neural Networks",
bookTitle="Domain Adaptation in Computer Vision Applications",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="189--209",
abstract="We introduce aRe-identification         representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transferDomain transfer         to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behavior can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new Gradient Reversal Layer. The resulting augmented architecture can be trained using standard backpropagation, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for image classification, where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.",
isbn="978-3-319-58347-1",
doi="10.1007/978-3-319-58347-1_10",
url="https://doi.org/10.1007/978-3-319-58347-1_10"
}

@misc{tensorflow,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@ARTICLE{mnist,
author={L. Deng},
journal={IEEE Signal Processing Magazine},
title={The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]},
year={2012},
volume={29},
number={6},
pages={141-142},
keywords={handwriting recognition;learning (artificial intelligence);optical character recognition;visual databases;Best of the Web;MNIST database;handwritten digit images;machine learning research;modified national institute of standards and technology;optical character recognition;Machine learning},
doi={10.1109/MSP.2012.2211477},
ISSN={1053-5888},
month={Nov},}

