<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Exploiting synthetic images for real-world image recognition</title>
<style>body { zoom:200%; }</style>
<style>.figure { font-size: smaller; float: right; max-width: 60%; margin: 2em; margin-top: 0; }</style>
</head>
<body>
<h1>
Exploiting synthetic images for real-world image recognition
</h1>
<p><a href="http://in.maxmaton.nl">Max Maton</a>, Jan van Gemert, Miriam Huijser, Osman Kayhan

<h2>
Abstract
</h2>
<p>
Creating big datasets is often difficult or expensive which causes people to augment their dataset with rendered images. This often fails to significantly improve accuracy due to a difference in distribution between real and rendered datasets.
This paper shows that the gap between synthetic and real-world image distributions can be closed by using GANs to convert the synthetic data to a dataset which has the same distribution as the real data. Training this GAN requires only a fraction of the dataset traditionally required to get a high classification accuracy. This converted data can subsequently be used to train a classifier with a higher accuracy than a classifier trained only on the real dataset.

<h2>
Introduction
</h2>
<div class="figure" id="figure1">
<img src="method.svg" width="300em"><br>
Figure 1. <i>Technique used to convert synthetic dataset to be used in training.</i>
</div>

<p>
Deep learning has revolutionised visual object recognition. Thanks to huge datasets and fast hardware (GPUs), current object recognition approaches have near-human accuracy.
<p>
Because creating big datasets is often very expensive, start to turn to rendered images to augment their datasets. However, training networks on rendered images may not achieve the desired accuracy due to a <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.151.7688">gap between synthetic and real image distributions</a>.
<p>
Another development in current research is the increased focus on Generative Adversarial Networks (GANs) to generate images that <a href="https://arxiv.org/pdf/1406.2661.pdf">look similar to the images they were trained on</a>.
<p>
In this paper the impact of the distribution gap between synthetic and real image distributions is decreased by using a GAN to modify rendered images to have the same distribution as real images. This technique is shown to be useful for inflating very small datasets to a level where they can be used to create more accurate classifiers.

<h2>
Related work
</h2>
<p>
Rendered data can sometimes be used to train networks, i.e. using <a href="https://arxiv.org/pdf/1511.07041.pdf">rendered images to segment images of indoor scenes</a>, <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.151.7688">font character classification trained on interpolated real samples</a> and <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/w23/html/Abbasnejad_Using_Synthetic_Data_ICCV_2017_paper.html">facial expression analysis using rendered faces</a>. These networks are trained by creating a rendered dataset that is as close as possible in statistical distribution as the real dataset. Generally this is very difficult or expensive to achieve.

<p>
The problem of creating rendered datasets with a distribution close to a real dataset can be solved by using domain adaptation. This is generally done using Generative Adversarial Networks [<a href="https://arxiv.org/pdf/1703.05192.pdf">1</a>, <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Bousmalis_Unsupervised_Pixel-Level_Domain_CVPR_2017_paper.pdf">2</a>, <a href="https://link-springer-com.tudelft.idm.oclc.org/book/10.1007%2F978-3-319-58347-1">3</a>] trained to create samples based on images in the rendered dataset that are indistinguisable from images in the real dataset. We decided to perform this research on the <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Bousmalis_Unsupervised_Pixel-Level_Domain_CVPR_2017_paper.pdf">GAN as described by Bousmalis et al</a> due to the available <a href="https://www.tensorflow.org/">TensorFlow</a> implementation.

<h2>
Inflating datasets using synthetic data and GANs
</h2>
<div class="figure" id="figure-gan">
<img src="GAN.svg" width="300em"><br>
Figure _. <i>Overview of the GAN architecture.</i>
</div>
<p>
The basis of this research is a GAN that converts images from a rendered dataset distribution to an image that appears to come from the distribution of a real dataset. This GAN consists of three parts which are shown in figure _. The first part is a generator network that does the image conversion. The second part of the GAN is a discriminator, which tries to distinguish between the output of the generator and images from the real dataset. The third part of the GAN is a classifier that predicts the label of images coming from either the real dataset, the generator or the rendered dataset. All these networks are trained in parallel. The discriminator and classifier are trained to reduce the amount of misclassifications. The generator is trained to minimise the amount of pixels changed in the image, to minimise the loss in classification by the classifier and to try to fool the discriminator to classify the image as real.

<p>
To test the effects of inflating datasets when using this technique, we first trained the GAN with a real and rendered dataset. We then apply the trained GAN on the rendered dataset to create a new synthetic dataset. A combined dataset consisting of the synthetic dataset and the real dataset was used to train a second classifier.

<h2 id="approach">
Experiments
</h2>
<div class="figure" id="figure2">

<img src="images/0.npy.preview.0.png" width="28" height="28">
<img src="images/0.npy.preview.1000.png" width="28" height="28">
<img src="images/0.npy.preview.10000.png" width="28" height="28">
<img src="images/0.npy.preview.11000.png" width="28" height="28">
<img src="images/0.npy.preview.2000.png" width="28" height="28">
<img src="images/0.npy.preview.3000.png" width="28" height="28">
<img src="images/0.npy.preview.4000.png" width="28" height="28">
<img src="images/0.npy.preview.5000.png" width="28" height="28">
<img src="images/0.npy.preview.6000.png" width="28" height="28">
<img src="images/0.npy.preview.7000.png" width="28" height="28">
<img src="images/0.npy.preview.8000.png" width="28" height="28">
<img src="images/0.npy.preview.9000.png" width="28" height="28">
<br>
<img src="images/1.npy.preview.0.png" width="28" height="28">
<img src="images/1.npy.preview.1000.png" width="28" height="28">
<img src="images/1.npy.preview.10000.png" width="28" height="28">
<img src="images/1.npy.preview.11000.png" width="28" height="28">
<img src="images/1.npy.preview.2000.png" width="28" height="28">
<img src="images/1.npy.preview.3000.png" width="28" height="28">
<img src="images/1.npy.preview.4000.png" width="28" height="28">
<img src="images/1.npy.preview.5000.png" width="28" height="28">
<img src="images/1.npy.preview.6000.png" width="28" height="28">
<img src="images/1.npy.preview.7000.png" width="28" height="28">
<img src="images/1.npy.preview.8000.png" width="28" height="28">
<img src="images/1.npy.preview.9000.png" width="28" height="28">
<br>
<img src="images/2.npy.preview.0.png" width="28" height="28">
<img src="images/2.npy.preview.1000.png" width="28" height="28">
<img src="images/2.npy.preview.10000.png" width="28" height="28">
<img src="images/2.npy.preview.11000.png" width="28" height="28">
<img src="images/2.npy.preview.2000.png" width="28" height="28">
<img src="images/2.npy.preview.3000.png" width="28" height="28">
<img src="images/2.npy.preview.4000.png" width="28" height="28">
<img src="images/2.npy.preview.5000.png" width="28" height="28">
<img src="images/2.npy.preview.6000.png" width="28" height="28">
<img src="images/2.npy.preview.7000.png" width="28" height="28">
<img src="images/2.npy.preview.8000.png" width="28" height="28">
<img src="images/2.npy.preview.9000.png" width="28" height="28">
<br>
<img src="images/3.npy.preview.0.png" width="28" height="28">
<img src="images/3.npy.preview.1000.png" width="28" height="28">
<img src="images/3.npy.preview.10000.png" width="28" height="28">
<img src="images/3.npy.preview.11000.png" width="28" height="28">
<img src="images/3.npy.preview.2000.png" width="28" height="28">
<img src="images/3.npy.preview.3000.png" width="28" height="28">
<img src="images/3.npy.preview.4000.png" width="28" height="28">
<img src="images/3.npy.preview.5000.png" width="28" height="28">
<img src="images/3.npy.preview.6000.png" width="28" height="28">
<img src="images/3.npy.preview.7000.png" width="28" height="28">
<img src="images/3.npy.preview.8000.png" width="28" height="28">
<img src="images/3.npy.preview.9000.png" width="28" height="28">
<br>
<img src="images/4.npy.preview.0.png" width="28" height="28">
<img src="images/4.npy.preview.1000.png" width="28" height="28">
<img src="images/4.npy.preview.10000.png" width="28" height="28">
<img src="images/4.npy.preview.11000.png" width="28" height="28">
<img src="images/4.npy.preview.2000.png" width="28" height="28">
<img src="images/4.npy.preview.3000.png" width="28" height="28">
<img src="images/4.npy.preview.4000.png" width="28" height="28">
<img src="images/4.npy.preview.5000.png" width="28" height="28">
<img src="images/4.npy.preview.6000.png" width="28" height="28">
<img src="images/4.npy.preview.7000.png" width="28" height="28">
<img src="images/4.npy.preview.8000.png" width="28" height="28">
<img src="images/4.npy.preview.9000.png" width="28" height="28">
<br>
<img src="images/5.npy.preview.0.png" width="28" height="28">
<img src="images/5.npy.preview.1000.png" width="28" height="28">
<img src="images/5.npy.preview.10000.png" width="28" height="28">
<img src="images/5.npy.preview.11000.png" width="28" height="28">
<img src="images/5.npy.preview.2000.png" width="28" height="28">
<img src="images/5.npy.preview.3000.png" width="28" height="28">
<img src="images/5.npy.preview.4000.png" width="28" height="28">
<img src="images/5.npy.preview.5000.png" width="28" height="28">
<img src="images/5.npy.preview.6000.png" width="28" height="28">
<img src="images/5.npy.preview.7000.png" width="28" height="28">
<img src="images/5.npy.preview.8000.png" width="28" height="28">
<img src="images/5.npy.preview.9000.png" width="28" height="28">
<br>
<img src="images/6.npy.preview.0.png" width="28" height="28">
<img src="images/6.npy.preview.1000.png" width="28" height="28">
<img src="images/6.npy.preview.10000.png" width="28" height="28">
<img src="images/6.npy.preview.11000.png" width="28" height="28">
<img src="images/6.npy.preview.2000.png" width="28" height="28">
<img src="images/6.npy.preview.3000.png" width="28" height="28">
<img src="images/6.npy.preview.4000.png" width="28" height="28">
<img src="images/6.npy.preview.5000.png" width="28" height="28">
<img src="images/6.npy.preview.6000.png" width="28" height="28">
<img src="images/6.npy.preview.7000.png" width="28" height="28">
<img src="images/6.npy.preview.8000.png" width="28" height="28">
<img src="images/6.npy.preview.9000.png" width="28" height="28">
<br>
<img src="images/7.npy.preview.0.png" width="28" height="28">
<img src="images/7.npy.preview.1000.png" width="28" height="28">
<img src="images/7.npy.preview.10000.png" width="28" height="28">
<img src="images/7.npy.preview.11000.png" width="28" height="28">
<img src="images/7.npy.preview.2000.png" width="28" height="28">
<img src="images/7.npy.preview.3000.png" width="28" height="28">
<img src="images/7.npy.preview.4000.png" width="28" height="28">
<img src="images/7.npy.preview.5000.png" width="28" height="28">
<img src="images/7.npy.preview.6000.png" width="28" height="28">
<img src="images/7.npy.preview.7000.png" width="28" height="28">
<img src="images/7.npy.preview.8000.png" width="28" height="28">
<img src="images/7.npy.preview.9000.png" width="28" height="28">
<br>
<img src="images/8.npy.preview.0.png" width="28" height="28">
<img src="images/8.npy.preview.1000.png" width="28" height="28">
<img src="images/8.npy.preview.10000.png" width="28" height="28">
<img src="images/8.npy.preview.11000.png" width="28" height="28">
<img src="images/8.npy.preview.2000.png" width="28" height="28">
<img src="images/8.npy.preview.3000.png" width="28" height="28">
<img src="images/8.npy.preview.4000.png" width="28" height="28">
<img src="images/8.npy.preview.5000.png" width="28" height="28">
<img src="images/8.npy.preview.6000.png" width="28" height="28">
<img src="images/8.npy.preview.7000.png" width="28" height="28">
<img src="images/8.npy.preview.8000.png" width="28" height="28">
<img src="images/8.npy.preview.9000.png" width="28" height="28">
<br>
<img src="images/9.npy.preview.0.png" width="28" height="28">
<img src="images/9.npy.preview.1000.png" width="28" height="28">
<img src="images/9.npy.preview.10000.png" width="28" height="28">
<img src="images/9.npy.preview.11000.png" width="28" height="28">
<img src="images/9.npy.preview.2000.png" width="28" height="28">
<img src="images/9.npy.preview.3000.png" width="28" height="28">
<img src="images/9.npy.preview.4000.png" width="28" height="28">
<img src="images/9.npy.preview.5000.png" width="28" height="28">
<img src="images/9.npy.preview.6000.png" width="28" height="28">
<img src="images/9.npy.preview.7000.png" width="28" height="28">
<img src="images/9.npy.preview.8000.png" width="28" height="28">
<img src="images/9.npy.preview.9000.png" width="28" height="28">
<br>
Figure 2. <i>Example images of the MNIST<sub>font</sub> dataset.</i>
</div>
<p>
We evaluated this technique on the <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a> together with a rendered dataset generated by rendering open source font digits in multiple rotations for all number classes.
<p>
The synthetic dataset contains samples from 149 <a href="fontlist.txt">fonts</a>, with each digit rendered having 47 variations each with a distinct rotation between -47 and 46 degrees. All images were normalised using the same algorithm used to normalise the MNIST samples. A random sample of 10,030 of those images were put in a test set and the remaining 60,000 images were used as the rendered dataset.
<p>
The GAN was modified to use 10% of the training data as a validation set instead of a constant 1,000 samples as this allowed for smaller sample sizes.

<p>
For multiple ratios r, where 0 &lt; r &lt; 1, we created the following datasets:
<dl>
<dt>MNIST<sub>original</sub></dt><dd><code>60,000 * r</code> images from the original MNIST dataset.</dd>
<dt>MNIST<sub>font</sub></dt><dd><code>60,000 * (1 - r)</code> images from the rendered font dataset.</dd>
</dl>

<p>
We used these two datasets to train the GAN using 375,000 steps with batchsize 32 and subsequently applied the trained GAN on the MNIST<sub>font</sub> dataset to create the MNIST<sub>GAN</sub> dataset. This made the MNIST<sub>GAN</sub> dataset have the exact same size as MNIST<sub>font</sub>.

<p>
With these datasets we trained five instances of the following classifiers:
<dl>
<dt>5 × C-MNIST<sub>original</sub></dt><dd>Classifier only trained on MNIST<sub>original</sub>.</dd>
<dt>5 × C-MNIST<sub>font</sub></dt><dd>Classifier only trained on MNIST<sub>font</sub>.</dd>
<dt>5 × C-MNIST<sub>GAN</sub></dt><dd>Classifier only trained on MNIST<sub>GAN</sub>.</dd>
<dt>5 × C-MNIST<sub>original+gan</sub></dt><dd>Classifier trained on MNIST<sub>original</sub> and MNIST<sub>GAN</sub>.</dd>
<dt>5 × C-MNIST<sub>original+font</sub></dt><dd>Classifier trained on MNIST<sub>original</sub> and MNIST<sub>font</sub>.</dd>
</dl>

<p>
Each classifier was tested on the MNIST test set which resulted in an classification accuracy.

<h2 id="results">
Results
</h2>

<div class="figure" id="figure3">
<div>
<i><code>r = 0.001</code> - top is from MNIST<sub>font</sub>, bottom is the generated image in MNIST<sub>GAN</sub>.</i><br>
<img src="results/0.001/0.001-original-0.png" alt="0">
<img src="results/0.001/0.001-original-1.png" alt="1">
<img src="results/0.001/0.001-original-2.png" alt="2">
<img src="results/0.001/0.001-original-3.png" alt="3">
<img src="results/0.001/0.001-original-4.png" alt="4">
<img src="results/0.001/0.001-original-5.png" alt="5">
<img src="results/0.001/0.001-original-6.png" alt="6">
<img src="results/0.001/0.001-original-7.png" alt="7">
<img src="results/0.001/0.001-original-8.png" alt="8">
<img src="results/0.001/0.001-original-9.png" alt="9">

<br>
<img src="results/0.001/0.001-converted-0.png" alt="0">
<img src="results/0.001/0.001-converted-1.png" alt="1">
<img src="results/0.001/0.001-converted-2.png" alt="2">
<img src="results/0.001/0.001-converted-3.png" alt="3">
<img src="results/0.001/0.001-converted-4.png" alt="4">
<img src="results/0.001/0.001-converted-5.png" alt="5">
<img src="results/0.001/0.001-converted-6.png" alt="6">
<img src="results/0.001/0.001-converted-7.png" alt="7">
<img src="results/0.001/0.001-converted-8.png" alt="8">
<img src="results/0.001/0.001-converted-9.png" alt="9">
</div>

<div>
<i><code>r = 0.007</code> - top is from MNIST<sub>font</sub>, bottom is the generated image in MNIST<sub>GAN</sub>.</i><br>
<img src="results/0.007/0.007-original-0.png" alt="0">
<img src="results/0.007/0.007-original-1.png" alt="1">
<img src="results/0.007/0.007-original-2.png" alt="2">
<img src="results/0.007/0.007-original-3.png" alt="3">
<img src="results/0.007/0.007-original-4.png" alt="4">
<img src="results/0.007/0.007-original-5.png" alt="5">
<img src="results/0.007/0.007-original-6.png" alt="6">
<img src="results/0.007/0.007-original-7.png" alt="7">
<img src="results/0.007/0.007-original-8.png" alt="8">
<img src="results/0.007/0.007-original-9.png" alt="9">

<br>
<img src="results/0.007/0.007-converted-0.png" alt="0">
<img src="results/0.007/0.007-converted-1.png" alt="1">
<img src="results/0.007/0.007-converted-2.png" alt="2">
<img src="results/0.007/0.007-converted-3.png" alt="3">
<img src="results/0.007/0.007-converted-4.png" alt="4">
<img src="results/0.007/0.007-converted-5.png" alt="5">
<img src="results/0.007/0.007-converted-6.png" alt="6">
<img src="results/0.007/0.007-converted-7.png" alt="7">
<img src="results/0.007/0.007-converted-8.png" alt="8">
<img src="results/0.007/0.007-converted-9.png" alt="9">
</div>

<div>
<i><code>r = 0.100</code> - top is from MNIST<sub>font</sub>, bottom is the generated image in MNIST<sub>GAN</sub>.</i><br>
<img src="results/0.100/0.100-original-0.png" alt="0">
<img src="results/0.100/0.100-original-1.png" alt="1">
<img src="results/0.100/0.100-original-2.png" alt="2">
<img src="results/0.100/0.100-original-3.png" alt="3">
<img src="results/0.100/0.100-original-4.png" alt="4">
<img src="results/0.100/0.100-original-5.png" alt="5">
<img src="results/0.100/0.100-original-6.png" alt="6">
<img src="results/0.100/0.100-original-7.png" alt="7">
<img src="results/0.100/0.100-original-8.png" alt="8">
<img src="results/0.100/0.100-original-9.png" alt="9">

<br>
<img src="results/0.100/0.100-converted-0.png" alt="0">
<img src="results/0.100/0.100-converted-1.png" alt="1">
<img src="results/0.100/0.100-converted-2.png" alt="2">
<img src="results/0.100/0.100-converted-3.png" alt="3">
<img src="results/0.100/0.100-converted-4.png" alt="4">
<img src="results/0.100/0.100-converted-5.png" alt="5">
<img src="results/0.100/0.100-converted-6.png" alt="6">
<img src="results/0.100/0.100-converted-7.png" alt="7">
<img src="results/0.100/0.100-converted-8.png" alt="8">
<img src="results/0.100/0.100-converted-9.png" alt="9">
</div>

<div>
<i><code>r = 0.700</code> - top is from MNIST<sub>font</sub>, bottom is the generated image in MNIST<sub>GAN</sub>.</i><br>
<img src="results/0.700/0.700-original-0.png" alt="0">
<img src="results/0.700/0.700-original-1.png" alt="1">
<img src="results/0.700/0.700-original-2.png" alt="2">
<img src="results/0.700/0.700-original-3.png" alt="3">
<img src="results/0.700/0.700-original-4.png" alt="4">
<img src="results/0.700/0.700-original-5.png" alt="5">
<img src="results/0.700/0.700-original-6.png" alt="6">
<img src="results/0.700/0.700-original-7.png" alt="7">
<img src="results/0.700/0.700-original-8.png" alt="8">
<img src="results/0.700/0.700-original-9.png" alt="9">

<br>
<img src="results/0.700/0.700-converted-0.png" alt="0">
<img src="results/0.700/0.700-converted-1.png" alt="1">
<img src="results/0.700/0.700-converted-2.png" alt="2">
<img src="results/0.700/0.700-converted-3.png" alt="3">
<img src="results/0.700/0.700-converted-4.png" alt="4">
<img src="results/0.700/0.700-converted-5.png" alt="5">
<img src="results/0.700/0.700-converted-6.png" alt="6">
<img src="results/0.700/0.700-converted-7.png" alt="7">
<img src="results/0.700/0.700-converted-8.png" alt="8">
<img src="results/0.700/0.700-converted-9.png" alt="9">
</div>
Figure 3. <i>Outputs from multiple runs of the GAN with different amounts of original training data. The network seems to be unable to keep the label consistent when there is not enough data.</i>
</div>

<p>
To validate whether the GAN was creating useful images, we looked at the resulting images in MNIST<sub>GAN</sub>:

<p>
From visual inspection it is apparent that the GAN has trouble keeping the labels consistent if there is not enough real training data. An example of this are the 1 and the 2 of <code>r = 0.007</code> in <a href="#figure3">figure 3</a>: they both result in something that looks like the same 8 instead of something that looks like a 1 or 2 respectively. This means that the classifier trained on this new dataset will receive two very similar samples with conflicting labels. We measured the accuracy on MNIST of C-MNIST<sub>GAN</sub> and found low accuracy for lower ratios. This supports the conclusion that there is an issue with labeling when the GAN is not trained with enough real data.

<h3>C-MNIST<sub>font</sub> performance</h3>
<div class="figure" id="figure4">
<img src="graph-font.png" width="90%"><br>
Figure 4. <i>Performance of C-MNIST<sub>font</sub> with varying amounts of MNIST<sub>font</sub> training data. Performance decreases with more training samples indicating that MNIST<sub>original</sub> and MNIST<sub>font</sub> have different characteristics.</i>
</div>
<p>
To make sure we actually test whether the GAN is able to close the distribution gap we looked at the accuracy of C-MNIST<sub>font</sub>. For this dataset the error shows an inverse relation between the amount of samples and the performance of the classifier, which can be seen in <a href="#figure4">figure 4</a>. This indicates that a gap exists between the rendered font dataset and the MNIST test dataset.

<h3>
C-MNIST<sub>original</sub> against C-MNIST<sub>original+GAN</sub>
</h3>
<div class="figure" id="figure5">
<img src="graph-results.png" width="90%"><br>
Figure 5. <i>Performance of classifiers with different amounts of MNIST training data. Training with the GAN data improves results after a minimal amount of original MNIST samples.</i>
</div>

<p>
We compared the accuracy of C-MNIST<sub>font</sub>, C-MNIST<sub>GAN</sub>, C-MNIST<sub>original</sub> and C-MNIST<sub>original+GAN</sub> to see whether training with the MNIST<sub>original+GAN</sub> dataset is better than training with one of the individual datasets. The result of this comparison can be seen in <a href="#figure5">figure 5</a>.

<p>
C-MNIST<sub>original+GAN</sub> is only more accurate than the C-MNIST<sub>original</sub> when the GAN  is trained on more than 385 real images (<code>r = 0.007</code>). There seems to be a minimum amount of real samples after which the GAN starts to produce meaningful data. 

<p>
Using MNIST as a test case, this technique is able to close the distribution gap even with as little 0.7% real data in the resulting dataset.

<div class="figure" id="figure-error-comparison">
<img src="graph-error-comparison.png" width="90%"><br>
Figure _. <i>Error of C-MNIST<sub>original+GAN</sub> compared to the error of C-MNIST<sub>original</sub>, lower is better. Lowest relative error was found at 5,500 MNIST samples.</i>
</div>

<p>We also compared the difference in wrong predictions between C-MNIST<sub>original</sub> and C-MNIST<sub>original+GAN</sub>. As is shown in figure _, the highest reduction in error was achieved with 5,500 MNIST images and 49,500 font images. This indicates that for MNIST, this technique becomes more effective when the ratio between real and rendered images becomes less extreme.

<h3>C-MNIST<sub>original</sub> against C-MNIST<sub>original+font</sub></h3>

<div class="figure" id="figure6">
<img src="graph-font-original-comparison.png" width="90%"><br>
Figure 6. <i>Performance of C-MNIST<sub>original+font</sub> versus the performance of C-MNIST<sub>original</sub> for varying amounts of training data. Performance is almost the same indicating no advantage to including font data in the training set directly.</i>
</div>

<p>
To confirm that the improved results of C-MNIST<sub>original+GAN</sub> can not be solely explained by the addition of the rendered font data, we compared C-MNIST<sub>original</sub> with C-MNIST<sub>original+font</sub> to see if the addition of font data increased the accuracy. From the results in <a href="#figure6">figure 6</a> we had to conclude that C-MNIST<sub>original+font</sub> has performance roughly equal to C-MNIST<sub>original</sub> at the ratios where C-MNIST<sub>original+GAN</sub> is more accurate. The addition of more font data cannot explain the additional accuracy of C-MNIST<sub>original+GAN</sub> indicating this increase is a property of the transformation made by the GAN.

<h2>
Discussion
</h2>
<p>
Due to a limited amount of time, no finetuning has been done on the other hyperparameters of the GAN. Optimizing these hyperparameters might further improve classification accuracy which should result in a further reduction in the amount of required samples from the real dataset.

<p>
We measured the performance of C-MNIST<sub>original+GAN</sub> versus C-MNIST<sub>original</sub>. It showed that even though the generated samples are not good samples for their target class, C-MNIST<sub>original+GAN</sub> quickly becomes much more accurate than C-MNIST<sub>original</sub> before the samples look like useful samples. This effect can partially be explained by results shown by Rolnick et al. that indicate that <a href="https://www.semanticscholar.org/paper/Training-Convolutional-Networks-with-Noisy-Labels-Sukhbaatar-Bruna/9f59d0a003558066d2ff4fc1c77f461b4d233663?tab=references">deep learning is robust to massive label noise</a>. This would mean that the network is still capable of learning higher level features from the mislabeled samples and is able to succesfully ignore the bad labeling.

<h2>
Conclusion
</h2>
<p>Our goal was to use a GAN to close the distribution gap between rendered and real datasets.

<p>
Using MNIST as an example, we were able to show that it's possible to create a rendered dataset. We've shown that a distribution gap exists between the dataset we created and the real MNIST dataset and were able to close this gap using our described technique.

<p>
We've shown that GANs can be used to inflate trainingsets by reducing the gap between synthetic and real datasets. Furthermore, they can do so with very little real training data. This technique can be very useful for problems where only a small real dataset is available.

<h1>Appendix</h1>
<ul>
<li><a href="results-mac-gen5.csv">Raw experiment results (csv)</a>
</ul>

</body>
</html>
